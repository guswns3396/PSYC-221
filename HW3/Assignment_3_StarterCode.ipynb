{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbupO2bS52ju"
   },
   "source": [
    "# **Machine Learning for Neuroimaging 2023 ‚Äî Assignment 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6k_SV9veBII"
   },
   "source": [
    "## **ABIDE I Functional Connectome Analysis**\n",
    "\n",
    "You have been provided with a resting-state fMRI connectome dataset of 120 individuals diagnosed with Autism Spectrum Disorder (ASD) and 120 typical controls. Each connectome, i.e., each subject‚Äôs brain network and properties, is encoded as an ùëÅ-by-ùëÅ correlation matrix ùêå, where $M_{i,j}$ is the functional connectivity (correlation in activation patterns) between region $i$ and region $j$.\n",
    "\n",
    "Note that this is the preprocessed version of ABIDE provided by the Preprocessed Connectome Project (PCP).\n",
    "\n",
    "For more information about this dataset's structure: http://preprocessed-connectomes-project.org/abide/\n",
    "\n",
    "*Cameron Craddock, Yassine Benhajali, Carlton Chu, Francois Chouinard, Alan Evans, Andr√°s Jakab, Budhachandra Singh Khundrakpam, John David Lewis, Qingyang Li, Michael Milham, Chaogan Yan, Pierre Bellec (2013). The Neuro Bureau Preprocessing Initiative: open sharing of preprocessed neuroimaging data and derivatives. In Neuroinformatics 2013, Stockholm, Sweden.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPUs4Vsp6sap"
   },
   "source": [
    "**Relevant Libraries**\n",
    "*  [NumPy](https://numpy.org/):https For numerical computing and handling multi-dimensional data.\n",
    "*  [Pandas](https://pandas.pydata.org/): For structured data operations and manipulations.\n",
    "*  [Matplotlib](https://matplotlib.org/): For creating static, interactive, and animated visualizations in Python.\n",
    "*  [scikit-learn](https://scikit-learn.org/stable/): For implementing machine learning algorithms.\n",
    "*  [nibabel](https://nipy.org/nibabel/): For reading and writing neuroimaging data formats.\n",
    "*  [nilearn](https://nilearn.github.io/stable/index.html): For advanced neuroimaging data manipulation and visualization.\n",
    "*  [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/): For writing and training Graph Neural Networks (GNNs).\n",
    "*  [DGL (Deep Graph Library)](https://www.dgl.ai/): For deep learning on GNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "H2ljMMms9tqg"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# @title Run to install needed packages.\n",
    "!pip install nilearn torch torchvision torchaudio torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "yVOPS-V3tYHZ"
   },
   "outputs": [],
   "source": [
    "# @title Load parcellated, connectivity matrices as our features, X\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "\n",
    "'''\n",
    "Load the connectivity matrices\n",
    "Note: each connectivity matrix (sample) was derived using the NiftiLabelsMasker\n",
    "and ConnectivityMeasure [with full Pearson's correlation] from nilearn and each\n",
    "sample was also standardized by z-score\n",
    "[i.e., zero mean scaled to unit variance w.r.t. sample std]\n",
    "'''\n",
    "\n",
    "data_path = './ABIDE_240.npz'\n",
    "# Load the data\n",
    "data = np.load(data_path)\n",
    "\n",
    "# Accessing the arrays\n",
    "connectomes = data['features']\n",
    "y_abide = data['labels']\n",
    "\n",
    "print(\"Data loaded from NPZ file.\")\n",
    "\n",
    "# Check the number of subject functional scans fetched\n",
    "print(f\"Number of subjects: {len(connectomes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m-s2tWmyfpIX"
   },
   "outputs": [],
   "source": [
    "from nilearn import plotting\n",
    "from nilearn import datasets\n",
    "\n",
    "# Retrieve AAL brain atlas for parcellation, more info here: https://www.sciencedirect.com/science/article/pii/S1053811901909784\n",
    "parcellations = datasets.fetch_atlas_aal()\n",
    "atlas_filename = parcellations.maps\n",
    "labels = parcellations.labels\n",
    "print(f\"Number of ROIs: {len(labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVLviv8C5mje"
   },
   "source": [
    "## Q1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3E4jpX10nFoW"
   },
   "source": [
    "Plot a random connectivity matrix from the dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rpREL05C4S7_"
   },
   "outputs": [],
   "source": [
    "cm_sample = connectomes[47]\n",
    "print(f\"Connectivity matrix shape: {cm_sample.shape}\")\n",
    "\n",
    "np.fill_diagonal(cm_sample, 0)\n",
    "\n",
    "plotting.plot_matrix(cm_sample, figure=(10, 8),\n",
    "                     labels=range(cm_sample.shape[-1]),\n",
    "                     vmax=0.8, vmin=-0.8, reorder=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDt1B2c_LU8Q"
   },
   "source": [
    "Now, you're ready to prepare the data for machine learning or statistical analysis.\n",
    "\n",
    "Note: If you are interested in using graph neural networks, feel free to use the below PyG starter code to create your custom PyG Dataset. Check the assignment description for links to related tutorials.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "hetxZvDwM8iR"
   },
   "outputs": [],
   "source": [
    "# @title [GraphML ONLY] Create custom PyG ConnectomeDataset\n",
    "import torch\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "\n",
    "class ConnectomeDataset(Dataset):\n",
    "    def __init__(self, connectivity_matrices, labels, task=\"classification\", transform=None, pre_transform=None):\n",
    "        super(ConnectomeDataset, self).__init__(None, transform, pre_transform)\n",
    "        self.connectivity_matrices = connectivity_matrices\n",
    "        self.labels = labels\n",
    "        self.task = task\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.connectivity_matrices)\n",
    "\n",
    "    def get(self, idx):\n",
    "        # Convert the connectivity matrix to edge index and edge attributes\n",
    "        connectivity_matrix = torch.tensor(self.connectivity_matrices[idx])\n",
    "        edge_index, edge_attr = dense_to_sparse(connectivity_matrix)\n",
    "\n",
    "        # Create a data object\n",
    "        data = Data(edge_index=edge_index, edge_attr=edge_attr)\n",
    "        data.x = connectivity_matrix.to(torch.float)\n",
    "        if self.task == \"classification\":\n",
    "          data.y = torch.tensor([self.labels[idx]-1], dtype=torch.long) # make labels start at 0\n",
    "        else:\n",
    "          data.y = torch.tensor([self.labels[idx]], dtype=torch.float)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXX-GuaFU5NY"
   },
   "source": [
    "### Example - GraphML Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AB1Fd0CQNTRt"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Instantiate the dataset\n",
    "abide_dataset = ConnectomeDataset(connectomes, y_abide)\n",
    "\n",
    "loader = DataLoader(abide_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(f'Number of graphs: {len(abide_dataset)}')\n",
    "print(\"Number of node features: \", abide_dataset.num_node_features)\n",
    "print(f'Number of edge features: {abide_dataset.num_edge_features}')\n",
    "\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abide_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* edge_index (LongTensor, optional) ‚Äì Graph connectivity in COO format with shape [2, num_edges]. (default: None)\n",
    "* edge_attr (torch.Tensor, optional) ‚Äì Edge feature matrix with shape [num_edges, num_edge_features]. (default: None)\n",
    "\n",
    "In this case there is only 1 edge feature which is just the weight of the edge (correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a GCN model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(abide_dataset.num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        # Apply global mean pooling to get a single vector for the whole graph\n",
    "        x = global_mean_pool(x, batch=data.batch)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "GCN_model = GCN()\n",
    "print(GCN_model)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "GCN_model = GCN_model.to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, GCN_model.parameters()),\n",
    "            lr=0.0001,\n",
    "            weight_decay=0.0001\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Train the model\n",
    "GCN_model.train()\n",
    "loss_history =[]\n",
    "\n",
    "for epoch in range(100):\n",
    "    # Initialize variables to track the loss and accuracy for each epoch\n",
    "    epoch_loss = 0.0\n",
    "    epoch_correct_predictions = 0\n",
    "    epoch_total_predictions = 0\n",
    "\n",
    "    # Loop over each batch from the data loader\n",
    "    for batch in loader:\n",
    "        # Move batch to device\n",
    "        batch = batch.to(device)\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        out = GCN_model(batch)\n",
    "        # Calculate loss\n",
    "        loss = criterion(out, batch.y)\n",
    "        epoch_loss += loss.item()\n",
    "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "        # Calculate the number of correct predictions\n",
    "        predictions = out.argmax(dim=1)\n",
    "        epoch_correct_predictions += (predictions == batch.y).sum().item()\n",
    "        epoch_total_predictions += len(batch)\n",
    "\n",
    "    # Calculate the average loss and accuracy for the epoch\n",
    "    epoch_loss /= len(loader)\n",
    "    loss_history.append(epoch_loss)\n",
    "    epoch_accuracy = epoch_correct_predictions / epoch_total_predictions\n",
    "\n",
    "    if epoch % 10 == 0: # Print every 10 epochs\n",
    "        print(f'Epoch {epoch+1}, Loss: {epoch_loss:.3f}, Accuracy: {epoch_accuracy:.3f}')\n",
    "\n",
    "# Plot training loss history over epochs\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtgTMnc_JFGM"
   },
   "source": [
    "## Q2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7VfsCZRJiEL"
   },
   "source": [
    "### Example - Graph Network Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0PoKW_nsJEZD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 'cm_sample' is a random connectome from our dataset\n",
    "weights = cm_sample[np.triu_indices_from(cm_sample, k=1)]  # Extract upper triangle to avoid duplication\n",
    "\n",
    "# Plotting the distribution of weights\n",
    "plt.hist(weights, bins=30, alpha=0.7)\n",
    "plt.title(\"Distribution of Edge Weights\")\n",
    "plt.xlabel(\"Weight (Correlation Value)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBBah-dJLUL7"
   },
   "source": [
    "### Example - Network Community Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FpuWInrRLcYj"
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import community.community_louvain as community_louvain\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import colormaps\n",
    "\n",
    "'''Note: networkx expects non-negative edge weights for community detection;\n",
    "convert connectome edge weights to absolute value (changes edge weight\n",
    "information to strength only and not whether positively or negatively\n",
    "correlated.)\n",
    "'''\n",
    "# Find the absolute maximum value in the connectome for scaling\n",
    "abs_cm_sample = np.abs(cm_sample)\n",
    "\n",
    "G = nx.from_numpy_array(abs_cm_sample)\n",
    "\n",
    "# Use the Louvain method for community detection\n",
    "partition = community_louvain.best_partition(G, weight='weight')\n",
    "\n",
    "# Visualize the communities\n",
    "pos = nx.spring_layout(G)  # Positioning of the nodes\n",
    "cmap = colormaps['viridis']\n",
    "\n",
    "for com in set(partition.values()):\n",
    "    list_nodes = [nodes for nodes in partition.keys() if partition[nodes] == com]\n",
    "    nx.draw_networkx_nodes(G, pos, list_nodes, node_size=20,\n",
    "                           node_color=[cmap(com / (max(partition.values()) + 1))])\n",
    "\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
