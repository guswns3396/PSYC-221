{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbupO2bS52ju"
      },
      "source": [
        "# **Machine Learning for Neuroimaging: Connectome Analysis with Python**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWBHb9jm6H8x"
      },
      "source": [
        "## **Introduction**\n",
        "### **Welcome and Overview**\n",
        "Welcome everyone to this tutorial session on Machine Learning for Neuroimaging.\n",
        "\n",
        "**Objective:** Today, we will explore applying machine learning to analyzing connectomes using Python libraries.\n",
        "\n",
        "**Connectomes:** A connectome is a comprehensive map of neural connections in the brain. Studying connectomes can help us understand the intricate organization of neural (brain) networks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPUs4Vsp6sap"
      },
      "source": [
        "## **Python Libraries for Connectome Analysis (10 minutes)**\n",
        "### **Essential Libraries**\n",
        "*  [NumPy](https://numpy.org/):https For numerical computing and handling multi-dimensional data.\n",
        "*  [Pandas](https://pandas.pydata.org/): For structured data operations and manipulations.\n",
        "*  [Matplotlib](https://matplotlib.org/): For creating static, interactive, and animated visualizations in Python.\n",
        "*  [scikit-learn](https://scikit-learn.org/stable/): For implementing machine learning algorithms.\n",
        "*  [nibabel](https://nipy.org/nibabel/): For reading and writing neuroimaging data formats.\n",
        "*  [nilearn](https://nilearn.github.io/stable/index.html): For advanced neuroimaging data manipulation and visualization.\n",
        "*  [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/): For writing and training Graph Neural Networks (GNNs).\n",
        "*  [DGL (Deep Graph Library)](https://www.dgl.ai/): For deep learning on GNNs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "H2ljMMms9tqg"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# @title Run to install needed packages.\n",
        "!pip install nilearn torch torchvision torchaudio torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgAyyBKJ5k-T"
      },
      "outputs": [],
      "source": [
        "# Importing required libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "import nilearn\n",
        "from nilearn import plotting\n",
        "from nilearn import datasets\n",
        "\n",
        "\n",
        "# Load a sample dataset provided by nilearn\n",
        "# Here we will use the MNI152 template which is a standard MRI brain template\n",
        "mni152_template = datasets.load_mni152_template()\n",
        "\n",
        "# Visualize the template\n",
        "plotting.plot_img(mni152_template, title=\"MNI152 Template\")\n",
        "\n",
        "# Fetch a dataset of functional connectomes\n",
        "# We will use the nilearn datasets.fetch_atlas_msdl function to fetch atlas with group brain parcellations\n",
        "msdl_atlas_dataset = datasets.fetch_atlas_msdl()\n",
        "msdl_atlas = msdl_atlas_dataset.maps\n",
        "\n",
        "# Visualize the atlas\n",
        "plotting.plot_prob_atlas(msdl_atlas, title='Multiple Sclerosis Detection Atlas')\n",
        "\n",
        "# Fetch resting-state functional connectivity datasets\n",
        "# We will use the nilearn datasets.fetch_adhd function to fetch some resting-state functional connectivity data\n",
        "adhd_dataset = datasets.fetch_adhd(n_subjects=1)\n",
        "\n",
        "# Print basic information on the dataset\n",
        "print('First subject functional nifti images (4D) are at: %s' %\n",
        "      adhd_dataset.func[0])  # 4D data\n",
        "\n",
        "# Load the functional connectivity data of the first subject\n",
        "func_filename = adhd_dataset.func[0]\n",
        "func_data = nib.load(func_filename)\n",
        "\n",
        "# Visualize the functional connectivity data\n",
        "# We will use the mean image as the background to overlay on\n",
        "mean_func_img = nilearn.image.mean_img(func_filename)\n",
        "plotting.plot_epi(mean_func_img, title='Resting state functional connectivity')\n",
        "\n",
        "# Show plots (if not automatically displayed)\n",
        "plotting.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YQ_SR62-1WA"
      },
      "source": [
        "# **Introduction to Graph Neural Networks (GNNs)**\n",
        "\n",
        "Graph Neural Networks (GNNs) are a category of neural networks designed to perform inference on data that can be structured as graphs. They are particularly powerful for tasks where the data is inherently graph-structured, such as social networks, molecular structures, and, notably, neuroimaging data where the brain's functional and structural connectivity patterns can be represented as graphs.\n",
        "\n",
        "## **Key Concepts**\n",
        "### Node\n",
        "A node in a graph represents an entity. For instance, in neuroimaging, a node could represent a brain region. In mathematical terms, a node is denoted as $v$ in a graph $G$.\n",
        "\n",
        "### Edge\n",
        "An edge represents a relationship or connection between two nodes. In neuroimaging, an edge could represent a functional or structural connectivity *(e.g. Pearson's correlation cofficient)* between two brain regions. Mathematically, an edge is denoted as $(u, v)$, connecting node $u$ to node $v$.\n",
        "\n",
        "### Graph\n",
        "A graph is defined by a set of nodes and a set of edges. Each edge connects a pair of nodes. A graph $G$ is typically defined as $G = (V, E)$, where $V$ is the set of nodes and $E$ is the set of edges.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7D4qjFZAzzj"
      },
      "source": [
        "## **Examples of Graph Neural Networks**\n",
        "\n",
        "### **Graph Convolutional Networks (GCNs)**\n",
        "GCNs generalize convolutional neural networks (CNNs) to work on graph data. They operate by aggregating information from a node's neighbors to learn a representation of the node.\n",
        "\n",
        "**GCN Operation**\n",
        "\n",
        "The basic operation of a GCN on a node $v$ can be described by the following equation:\n",
        "\n",
        "$$\n",
        "h_v^{(l+1)} = \\sigma \\left( W^{(l)} \\sum_{u \\in \\mathcal{N}(v)} \\frac{1}{c_{uv}} h_u^{(l)} + b^{(l)} \\right)\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $h_v^{(l+1)}$ is the feature representation of node $v$ at layer $l+1$.\n",
        "- $\\sigma$ is a non-linear activation function, such as ReLU.\n",
        "- $W^{(l)}$ and $b^{(l)}$ are the trainable weight and bias at layer $l$.\n",
        "- $\\mathcal{N}(v)$ is the set of neighbors of $v$.\n",
        "- $c_{uv}$ is a normalization constant (often the degree of the node).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cont2L_NCTYK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "\n",
        "def get_y(dataset):\n",
        "    \"\"\"\n",
        "    Get the y values from a list of Data objects.\n",
        "    \"\"\"\n",
        "    y = []\n",
        "    for d in dataset:\n",
        "        y.append(d.y.numpy())\n",
        "    return np.array(y)\n",
        "\n",
        "# Load a dataset\n",
        "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(\"Number and count of classes: \", np.unique(get_y(dataset).flatten(), return_counts=True))\n",
        "print(\"Number of node features: \", dataset.num_node_features)\n",
        "print(f'Number of edge features: {dataset.num_edge_features}')\n",
        "\n",
        "\n",
        "# Define a GCN model\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(dataset.num_node_features, 16)\n",
        "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kmxOK96sFGb"
      },
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "GCN_model = GCN()\n",
        "print(GCN_model)\n",
        "\n",
        "# Move to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "GCN_model = GCN_model.to(device)\n",
        "data = dataset[0].to(device)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(GCN_model.parameters(), lr=0.01)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzYjDXdNsIi9"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "GCN_model.train()\n",
        "loss_history =[]\n",
        "\n",
        "for epoch in range(200):\n",
        "    # Initialize variables to track the loss and accuracy for each epoch\n",
        "    epoch_loss = 0.0\n",
        "    epoch_correct_predictions = 0\n",
        "    epoch_total_predictions = 0\n",
        "\n",
        "    # Loop over each batch from the data loader\n",
        "    for batch in dataset:\n",
        "        # Move batch to device\n",
        "        batch = batch.to(device)\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        out = GCN_model(batch)\n",
        "        # Calculate loss\n",
        "        loss = criterion(out, data.y)\n",
        "        epoch_loss += loss.item()\n",
        "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # Update the weights\n",
        "        optimizer.step()\n",
        "        # Calculate the number of correct predictions\n",
        "        predictions = out.argmax(dim=1)\n",
        "        epoch_correct_predictions += (predictions == data.y).sum().item()\n",
        "        epoch_total_predictions += len(data.train_mask)\n",
        "\n",
        "    # Calculate the average loss and accuracy for the epoch\n",
        "    epoch_loss /= len(dataset)\n",
        "    loss_history.append(epoch_loss)\n",
        "    epoch_accuracy = epoch_correct_predictions / epoch_total_predictions\n",
        "\n",
        "    if epoch % 10 == 0: # Print every 10 epochs\n",
        "        print(f'Epoch {epoch+1}, Loss: {epoch_loss:.3f}, Accuracy: {epoch_accuracy:.3f}')\n",
        "\n",
        "# Plot training loss history over epochs\n",
        "plt.plot(loss_history)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss over Epochs')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c17lACHtA2bb"
      },
      "source": [
        "### **Graph Attention Networks (GATs)**\n",
        "GATs introduce the mechanism of attention to GNNs by each node to weigh its neighboring nodes' contributions differently.\n",
        "\n",
        "**GAT Operation**\n",
        "\n",
        "The key operation of a GAT can be described by the following equation:\n",
        "\n",
        "$$\n",
        "h_v^{(l+1)} = \\sigma \\left( \\sum_{u \\in \\mathcal{N}(v)} \\alpha_{uv} W^{(l)} h_u^{(l)} \\right)\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $\\alpha_{uv}$ is the attention coefficient that indicates the importance of node $u$'s features to node $v$.\n",
        "- $W^{(l)}$ is the trainable weight matrix at layer $l$.\n",
        "- $h_u^{(l)}$ is the feature representation of node $u$ at layer $l$.\n",
        "- The attention coefficients $\\alpha_{uv}$ are typically computed using a parametric function of the node features, which is learned during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0BFc4e4IPIU"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import GATConv\n",
        "\n",
        "# Define a GAT model\n",
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GAT, self).__init__()\n",
        "        self.conv1 = GATConv(dataset.num_node_features, 8, heads=8, dropout=0.6)\n",
        "        self.conv2 = GATConv(8 * 8, dataset.num_classes, heads=1, concat=True, dropout=0.6)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.dropout(x, p=0.6, training=self.training)\n",
        "        x = F.elu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=0.6, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Initialize the GAT model\n",
        "gat_model = GAT()\n",
        "print(gat_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_L5kzZaDYFz"
      },
      "outputs": [],
      "source": [
        "# Move to GPU if available\n",
        "gat_model = gat_model.to(device)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(gat_model.parameters(), lr=0.01)\n",
        "\n",
        "# Train the GAT model\n",
        "gat_model.train()\n",
        "loss_history =[]\n",
        "\n",
        "for epoch in range(200):\n",
        "    # Initialize variables to track the loss and accuracy for each epoch\n",
        "    epoch_loss = 0.0\n",
        "    epoch_correct_predictions = 0\n",
        "    epoch_total_predictions = 0\n",
        "\n",
        "    # Loop over each batch from the data loader\n",
        "    for batch in dataset:\n",
        "        # Move batch to device\n",
        "        batch = batch.to(device)\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        out = gat_model(batch)\n",
        "        # Calculate loss\n",
        "        loss = criterion(out, data.y)\n",
        "        epoch_loss += loss.item()\n",
        "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # Update the weights\n",
        "        optimizer.step()\n",
        "        # Calculate the number of correct predictions\n",
        "        predictions = out.argmax(dim=1)\n",
        "        epoch_correct_predictions += (predictions == data.y).sum().item()\n",
        "        epoch_total_predictions += len(data.train_mask)\n",
        "\n",
        "    # Calculate the average loss and accuracy for the epoch\n",
        "    epoch_loss /= len(dataset)\n",
        "    loss_history.append(epoch_loss)\n",
        "    epoch_accuracy = epoch_correct_predictions / epoch_total_predictions\n",
        "\n",
        "    if epoch % 10 == 0: # Print every 10 epochs\n",
        "        print(f'Epoch {epoch+1}, Loss: {epoch_loss:.3f}, Accuracy: {epoch_accuracy:.3f}')\n",
        "\n",
        "# Plot training loss history over epochs\n",
        "plt.plot(loss_history)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss over Epochs')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrYT8UU3D9De"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Test GAT model on the first sample in the dataset\n",
        "gat_model.eval()\n",
        "_, pred = gat_model(data).max(dim=1)\n",
        "\n",
        "# Get the embeddings\n",
        "embeddings = gat_model.conv1(data.x, data.edge_index)\n",
        "\n",
        "# Reduce the embeddings to 2 dimensions using t-SNE\n",
        "tsne = TSNE(n_components=2)\n",
        "embeddings_2d = tsne.fit_transform(embeddings.detach().cpu().numpy())\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=pred.cpu().numpy(), cmap='Set1')\n",
        "plt.legend(*scatter.legend_elements(), title=\"Classes\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-zQkl36JBtk"
      },
      "source": [
        "*We observe good separation of all 7 classes due to a high classification performance of the GAT model.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsOgo6KFDcG5"
      },
      "source": [
        "## **Applications to Neuroimaging**: ABIDE and ADHD Case Studies\n",
        "Preface: In neuroimaging, GNNs can be used to analyze brain connectivity patterns. Each brain region can be represented as a node, and the connections (either structural or functional) between regions are the edges. GNNs can help in tasks such as:\n",
        "\n",
        "*  Classification of cognitive states or disorders.\n",
        "*  Regression tasks to predict behavioral or genetic traits.\n",
        "*  Clustering or community detection within brain networks.\n",
        "\n",
        "Case:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6k_SV9veBII"
      },
      "source": [
        "# ABIDE Classification & Regression Tasks\n",
        "\n",
        "The Autism Brain Imaging Data Exchange (ABIDE) dataset provides previously collected resting state functional magnetic resonance imaging datasets from 539 individuals with ASD and 573 typical controls for the purpose of data sharing in the broader scientific community. This grass-root initiative involved 16 international sites, sharing 20 samples yielding 1112 datasets composed of both MRI data and an extensive array of phenotypic information common across nearly all sites (see below).\n",
        "\n",
        "Note that this is the preprocessed version of ABIDE provided by the preprocess connectome projects (PCP).\n",
        "\n",
        "For more information about this dataset's structure: http://preprocessed-connectomes-project.github.io http://www.childmind.org/en/healthy-brain-network/abide/\n",
        "\n",
        "Nielsen, Jared A., et al. \"Multisite functional connectivity MRI classification of autism: ABIDE results.\" Frontiers in human neuroscience 7 (2013).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBxtJer80zhe"
      },
      "source": [
        "**Note:** The ABIDE dataaset consumes a lot of storage space so we will provide pre-parcellated connectivity matrices below. Let's run through the steps we used to do so on fewer subjects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWiLyQtNVyNt"
      },
      "outputs": [],
      "source": [
        "# Fetch the ABIDE dataset\n",
        "abide = datasets.fetch_abide_pcp(n_subjects=5, pipeline=\"cpac\",\n",
        "                                 derivatives=['func_preproc'],\n",
        "                                 quality_checked=True,\n",
        "                                 legacy_format=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYt3MIoudmTc"
      },
      "outputs": [],
      "source": [
        "# Store the filenames of the functional scans\n",
        "fmri_filenames = abide.func_preproc\n",
        "\n",
        "# Check the number of subject functional scans fetched\n",
        "print(f\"Number of subjects: {len(fmri_filenames)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZanovzxh9SH"
      },
      "source": [
        "We need to decide which parcellation to use for rs-fMRI data. We are going to use the AAL atlas (...)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-s2tWmyfpIX"
      },
      "outputs": [],
      "source": [
        "from nilearn import plotting\n",
        "\n",
        "# Retrieve brain atlas for parcellation\n",
        "parcellations = datasets.fetch_atlas_aal()\n",
        "atlas_filename = parcellations.maps\n",
        "labels = parcellations.labels\n",
        "print(f\"Number of ROIs: {len(labels)}\")\n",
        "\n",
        "# Plot atlas\n",
        "plotting.plot_roi(atlas_filename, draw_cross=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SBeiZr5IFGO"
      },
      "source": [
        "Select a single subject's scan to check information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stBQelB5VL2A"
      },
      "outputs": [],
      "source": [
        "sample_subject = abide.func_preproc[0]\n",
        "sample_subject"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3agR8SBmYIZ"
      },
      "source": [
        "Using NiftiLabelsMasker we will create a mask on our functional images with the labels of the chosen atlas and extract the time series in each ROI. Because the data is already preprocessed, we do not need to regress out any confounds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShbmiqTm2o_C"
      },
      "outputs": [],
      "source": [
        "from nilearn.input_data import NiftiLabelsMasker\n",
        "\n",
        "masker = NiftiLabelsMasker(labels_img=atlas_filename,\n",
        "                           standardize='zscore_sample',  #z-score each sample to zero mean scaled to unit variance w.r.t. sample std\n",
        "                           memory='nilearn_cache',\n",
        "                           verbose=0)\n",
        "\n",
        "time_series = masker.fit_transform(sample_subject)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E4jpX10nFoW"
      },
      "source": [
        "Now we are going to extract the connectivity matrix for each sample using the pre-defined masker."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpREL05C4S7_"
      },
      "outputs": [],
      "source": [
        "from nilearn.connectome import ConnectivityMeasure\n",
        "\n",
        "correlation_measure = ConnectivityMeasure(kind='correlation')\n",
        "connectivity_matrix = correlation_measure.fit_transform([time_series])[0]\n",
        "print(f\"Connectivity matrix shape: {connectivity_matrix.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45GXiIhuH56C"
      },
      "source": [
        "...and plot it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_vi4QfeplJ_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.fill_diagonal(connectivity_matrix, 0)\n",
        "\n",
        "plotting.plot_matrix(connectivity_matrix, figure=(10, 8),\n",
        "                     labels=range(time_series.shape[-1]),\n",
        "                     vmax=0.8, vmin=-0.8, reorder=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVOPS-V3tYHZ"
      },
      "outputs": [],
      "source": [
        "# @title Load parcellated, connectivity matrices as our features, X\n",
        "import numpy as np\n",
        "import pickle as pkl\n",
        "\n",
        "# Load the connectivity matrices\n",
        "conn_matrices = './ABIDE_conn_matrices.npz'\n",
        "X = np.load(conn_matrices)['a']\n",
        "\n",
        "with open('./abide_preproc_300.pkl', 'rb') as f:\n",
        "    abide = pkl.load(f)\n",
        "\n",
        "# Store the filenames of the functional scans\n",
        "fmri_filenames = abide.func_preproc\n",
        "\n",
        "# Check the number of subject functional scans fetched\n",
        "print(f\"Number of subjects: {len(fmri_filenames)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "798ROiKSn1Ca"
      },
      "source": [
        "Now we have the connectivity matrices for all subjects. Let's see the shape of our connectivity matrices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xvEMcuHIz5D"
      },
      "outputs": [],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHeuPmXuuOG-"
      },
      "source": [
        "Accompanying the data set is a csv file containing the phenotypic data. According to the Phenotypic Data Legend which can be downloaded [here](http://fcon_1000.projects.nitrc.org/indi/abide/abide_I.html), the column DX_GROUP has the information about the diagnostic group each participant is in. It is coded as:\n",
        "\n",
        "*   1 = Autism\n",
        "*   2 = Control\n",
        "\n",
        "Let's import the csv."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCuvTsu1uWgc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "phenotypic = pd.read_csv(\"./Phenotypic_V1_0b_preprocessed1.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jqVhvRxKhNf"
      },
      "source": [
        "Let's use the file names to get the right values from the DX_GROUP column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88PbRxgyKeSE"
      },
      "outputs": [],
      "source": [
        "file_ids = []\n",
        "\n",
        "# Get the file IDs from the file names\n",
        "for f in fmri_filenames:\n",
        "    file_ids.append(f[-27:-20])\n",
        "\n",
        "# Get labels for autism diagnosis => classification task\n",
        "y_asd = []\n",
        "for i in range(len(phenotypic)):\n",
        "    for j in range(len(file_ids)):\n",
        "        if file_ids[j] in phenotypic.FILE_ID[i]:\n",
        "            y_asd.append(phenotypic.DX_GROUP[i])\n",
        "\n",
        "# Get labels for full-scale IQ (FIQ) => regression task\n",
        "y_fiq = []\n",
        "for i in range(len(phenotypic)):\n",
        "    for j in range(len(file_ids)):\n",
        "        if file_ids[j] in phenotypic.FILE_ID[i]:\n",
        "            y_fiq.append(phenotypic.FIQ[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDt1B2c_LU8Q"
      },
      "source": [
        "Now, we're ready to prepare out data for graph learning. We will first create a custom PyTorch Geometric dataset to convert connectivity matrices, X to edge index and edge attributes for our graphs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hetxZvDwM8iR"
      },
      "outputs": [],
      "source": [
        "# @title Create custom PyG ConnectomeDataset\n",
        "import torch\n",
        "from torch_geometric.data import Data, Dataset\n",
        "from torch_geometric.utils import dense_to_sparse\n",
        "class ConnectomeDataset(Dataset):\n",
        "    def __init__(self, connectivity_matrices, labels, task=\"classification\", transform=None, pre_transform=None):\n",
        "        super(ConnectomeDataset, self).__init__(None, transform, pre_transform)\n",
        "        self.connectivity_matrices = connectivity_matrices\n",
        "        self.labels = labels\n",
        "        self.task = task\n",
        "\n",
        "    def len(self):\n",
        "        return len(self.connectivity_matrices)\n",
        "\n",
        "    def get(self, idx):\n",
        "        # Convert the connectivity matrix to edge index and edge attributes\n",
        "        connectivity_matrix = torch.tensor(self.connectivity_matrices[idx])\n",
        "        edge_index, edge_attr = dense_to_sparse(connectivity_matrix)\n",
        "\n",
        "        # Create a data object\n",
        "        data = Data(edge_index=edge_index, edge_attr=edge_attr)\n",
        "        data.x = connectivity_matrix.to(torch.float)\n",
        "        if self.task == \"classifiction\":\n",
        "          data.y = torch.tensor([self.labels[idx]-1], dtype=torch.long) # make labels start at 0\n",
        "        else:\n",
        "          data.y = torch.tensor([self.labels[idx]], dtype=torch.float)\n",
        "        return data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The labels contain diagnostic group each participant is in. It is coded as:\n",
        "\n",
        "*   1 = Autism Spectrum Disorder (ASD)\n",
        "*   2 = Control\n",
        "\n",
        "**Let's re-index this to 0=ASD and 1=Control due to zero-indexed systems required for ML and other data processing software.**"
      ],
      "metadata": {
        "id": "dBILuT3WMv2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Adjust labels to start from 0\n",
        "y_asd = np.array(y_asd)\n",
        "y_asd = y_asd - 1\n",
        "\n",
        "# Print label classes and counts\n",
        "print(Counter(y_asd))"
      ],
      "metadata": {
        "id": "GR9WAQVMMp5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification Task"
      ],
      "metadata": {
        "id": "dXX-GuaFU5NY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AB1Fd0CQNTRt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch_geometric.transforms as T\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# Instantiate the dataset\n",
        "abide_dataset_asd = ConnectomeDataset(X, y_asd)\n",
        "\n",
        "loader = DataLoader(abide_dataset_asd, batch_size=32, shuffle=True)\n",
        "\n",
        "print(f'Number of graphs: {len(abide_dataset_asd)}')\n",
        "print(\"Number and count of classes: \", np.unique(get_y(abide_dataset_asd).flatten(), return_counts=True))\n",
        "print(\"Number of node features: \", abide_dataset_asd.num_node_features)\n",
        "print(f'Number of edge features: {abide_dataset_asd.num_edge_features}')\n",
        "\n",
        "num_classes = 2\n",
        "\n",
        "# Define a GCN model\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(abide_dataset_asd.num_node_features, 16)\n",
        "        self.conv2 = GCNConv(16, num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        # Apply global mean pooling to get a single vector for the whole graph\n",
        "        x = global_mean_pool(x, batch=data.batch)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgmPKaJLOaks"
      },
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "GCN_model = GCN()\n",
        "print(GCN_model)\n",
        "\n",
        "# Move to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "GCN_model = GCN_model.to(device)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(\n",
        "            filter(lambda p: p.requires_grad, GCN_model.parameters()),\n",
        "            lr=0.0001,\n",
        "            weight_decay=0.0001\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRKZfuy3OfMu"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "GCN_model.train()\n",
        "loss_history =[]\n",
        "\n",
        "for epoch in range(20):\n",
        "    # Initialize variables to track the loss and accuracy for each epoch\n",
        "    epoch_loss = 0.0\n",
        "    epoch_correct_predictions = 0\n",
        "    epoch_total_predictions = 0\n",
        "\n",
        "    # Loop over each batch from the data loader\n",
        "    for batch in loader:\n",
        "        # Move batch to device\n",
        "        batch = batch.to(device)\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        out = GCN_model(batch)\n",
        "        # Calculate loss\n",
        "        loss = criterion(out, batch.y)\n",
        "        epoch_loss += loss.item()\n",
        "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # Update the weights\n",
        "        optimizer.step()\n",
        "        # Calculate the number of correct predictions\n",
        "        predictions = out.argmax(dim=1)\n",
        "        epoch_correct_predictions += (predictions == batch.y).sum().item()\n",
        "        epoch_total_predictions += len(batch)\n",
        "\n",
        "    # Calculate the average loss and accuracy for the epoch\n",
        "    epoch_loss /= len(loader)\n",
        "    loss_history.append(epoch_loss)\n",
        "    epoch_accuracy = epoch_correct_predictions / epoch_total_predictions\n",
        "\n",
        "    if epoch % 10 == 0: # Print every 10 epochs\n",
        "        print(f'Epoch {epoch+1}, Loss: {epoch_loss:.3f}, Accuracy: {epoch_accuracy:.3f}')\n",
        "\n",
        "# Plot training loss history over epochs\n",
        "plt.plot(loss_history)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss over Epochs')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparison to Baseline"
      ],
      "metadata": {
        "id": "xU99pSEcT74m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare our model's performance to baseline traditional ML methods."
      ],
      "metadata": {
        "id": "FRGwpl8VTsGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, # x\n",
        "                                                  y_asd, # y\n",
        "                                                  test_size = 0.4, # 60%/40% split\n",
        "                                                  shuffle = True, # shuffle dataset before splitting\n",
        "                                                  stratify = y_asd,  # keep distribution of ASD consistent between sets\n",
        "                                                  random_state = 123 # same split each time\n",
        "                                                 )"
      ],
      "metadata": {
        "id": "CairYekeTw5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear SVC\n",
        "The classifier that is going to be used here is going to be a Linear Support Vector Classification (SVC). We will use cross-validation to estimate our accuracy."
      ],
      "metadata": {
        "id": "UiMuD6Y6T-7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
        "\n",
        "l_svc = LinearSVC(max_iter=100000) # more iterations than the default\n",
        "l_svc.fit(X_train, y_train)\n",
        "\n",
        "# predict\n",
        "y_pred_svc = cross_val_predict(l_svc, X_train, y_train, cv=10)\n",
        "# scores\n",
        "acc_svc = cross_val_score(l_svc, X_train, y_train, cv=10)\n",
        "\n",
        "print(\"Accuracy:\", acc_svc)\n",
        "print(\"Mean accuracy:\", acc_svc.mean())"
      ],
      "metadata": {
        "id": "Roxx6gyKT6VX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regression Task"
      ],
      "metadata": {
        "id": "qTY6z1npU0sR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJ_f-Mx0JlmP"
      },
      "source": [
        "Now, we'll do a regression task for predicting the full-scale intelligence (FIQ) of the subjects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZ-1jOb2Jk76"
      },
      "outputs": [],
      "source": [
        "# Use the dataset with a DataLoader for batching\n",
        "abide_dataset_fiq = ConnectomeDataset(X, y_fiq)\n",
        "train_loader = DataLoader(abide_dataset_fiq[:255], batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(abide_dataset_fiq[255:], batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZlU3O5b7V0O"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "num_classes = 1\n",
        "# Define a RegGNN model\n",
        "class RegGNN(nn.Module):\n",
        "    '''Regression using a DenseGCNConv layer from pytorch geometric.\n",
        "\n",
        "       Layers in this model are identical to GCNConv.\n",
        "       Adapted from: https://github.com/basiralab/RegGNN/blob/main/proposed_method/RegGNN.py\n",
        "    '''\n",
        "\n",
        "    def __init__(self, hidden_dim=64):\n",
        "        super(RegGNN, self).__init__()\n",
        "\n",
        "        self.gc1 = GCNConv(abide_dataset_fiq.num_features, hidden_dim)\n",
        "        self.gc2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.LinearLayer = torch.nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = F.relu(self.gc1(x, edge_index))\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = F.relu(self.gc2(x, edge_index))\n",
        "        # Apply global mean pooling to get a single vector for the whole graph\n",
        "        x = global_mean_pool(x, data.batch)\n",
        "        x = self.LinearLayer(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUoi59s4LExU"
      },
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "RegGNN_model = RegGNN()\n",
        "print(RegGNN_model)\n",
        "\n",
        "# Move to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "RegGNN_model = RegGNN_model.to(device)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(GCN_model.parameters(), lr=0.01)\n",
        "# optimizer = torch.optim.Adam(\n",
        "#             filter(lambda p: p.requires_grad, RegGNN_model.parameters()),\n",
        "#             lr=0.0001,\n",
        "#             weight_decay=0.0001\n",
        "#         )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibuGS5_fMIGh"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "RegGNN_model.train()\n",
        "loss_history =[]\n",
        "\n",
        "for epoch in range(50):\n",
        "    # Initialize variables to track the loss and accuracy for each epoch\n",
        "    epoch_loss = 0.0\n",
        "    epoch_correct_predictions = 0\n",
        "    epoch_total_predictions = 0\n",
        "\n",
        "    # Loop over each batch from the data loader\n",
        "    for batch in train_loader:\n",
        "        # Move batch to device\n",
        "        batch = batch.to(device)\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        out = RegGNN_model(batch)\n",
        "        # Calculate loss\n",
        "        loss = criterion(out, batch.y)\n",
        "        epoch_loss += float(loss)\n",
        "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # Update the weights\n",
        "        optimizer.step()\n",
        "        # Calculate the number of correct predictions\n",
        "        predictions = out.argmax(dim=1)\n",
        "        epoch_correct_predictions += (predictions == batch.y).sum().item()\n",
        "        epoch_total_predictions += len(batch)\n",
        "\n",
        "    # Calculate the average loss and accuracy for the epoch\n",
        "    epoch_loss /= len(train_loader)\n",
        "    loss_history.append(epoch_loss)\n",
        "    epoch_accuracy = epoch_correct_predictions / epoch_total_predictions\n",
        "\n",
        "    if epoch % 10 == 0: # Print every 10 epochs\n",
        "        print(f'Epoch {epoch+1}, Loss: {epoch_loss:.3f}, Accuracy: {epoch_accuracy:.3f}')\n",
        "\n",
        "# Plot training loss history over epochs\n",
        "plt.plot(loss_history)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss over Epochs')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}